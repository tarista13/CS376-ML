{"metadata":{"colab":{"authorship_tag":"ABX9TyNS7mRS03a7VSFcbdUnYf/k","collapsed_sections":[],"include_colab_link":true,"name":"012-tokenization.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tokenization\n\nTask: Convert text to numbers; interpret subword tokenization.\n\nThere are various different ways of converting text to numbers. This assignment works with one popular approach: assign numbers to parts of words.","metadata":{"id":"3jc8Qlh1TEgC"}},{"cell_type":"markdown","source":"## Setup","metadata":{"id":"f8_8RWp3TX-8"}},{"cell_type":"markdown","source":"We'll be using the HuggingFace Transformers library, which provides a (mostly) consistent interface to many different language models. We'll focus on the OpenAI GPT-2 model, famous for OpenAI's assertion that it was \"too dangerous\" to release in full.\n\n- [Documentation](https://huggingface.co/transformers/model_doc/gpt2.html) for the model and tokenizer.\n- [Model Card](https://github.com/openai/gpt-2/blob/master/model_card.md) for GPT-2.","metadata":{"id":"aUvTIxyWTdBF"}},{"cell_type":"markdown","source":"The `transformers` library is pre-installed on many systems, but in case you need to install it, you can run the following cell.","metadata":{}},{"cell_type":"code","source":"# Uncomment the following line to install the transformers library\n#!pip install -q transformers","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vWy--2nwhWPy","outputId":"44b8e674-7e8b-4cf6-a1e9-1f8d62740382","execution":{"iopub.status.busy":"2024-03-15T15:07:15.873647Z","iopub.execute_input":"2024-03-15T15:07:15.875172Z","iopub.status.idle":"2024-03-15T15:07:15.904359Z","shell.execute_reply.started":"2024-03-15T15:07:15.875112Z","shell.execute_reply":"2024-03-15T15:07:15.902533Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import tensor","metadata":{"id":"osKgPaDwhaN4","execution":{"iopub.status.busy":"2024-03-15T15:07:15.906780Z","iopub.execute_input":"2024-03-15T15:07:15.907323Z","iopub.status.idle":"2024-03-15T15:07:20.078963Z","shell.execute_reply.started":"2024-03-15T15:07:15.907274Z","shell.execute_reply":"2024-03-15T15:07:20.077808Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Download and load the model\n\nThis cell downloads the model and tokenizer, and loads them into memory.","metadata":{"id":"UiNKbIh8hyDg"}},{"cell_type":"code","source":"# https://huggingface.co/docs/transformers/en/generation_strategies\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, set_seed\nmodel_name = \"openai-community/gpt2\"\n# Here's a few larger models you could try:\n# model_name = \"EleutherAI/pythia-1.4b-deduped\"\n# model_name = \"google/gemma-2b\"\n# model_name = \"google/gemma-2b-it\"\n# Note: you'll need to accept the license agreement on https://huggingface.co/google/gemma-7b to use Gemma models\ntokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n\n# add the EOS token as PAD token to avoid warnings\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nif model.generation_config.pad_token_id is None:\n    model.generation_config.pad_token_id = model.generation_config.eos_token_id\nstreamer = TextStreamer(tokenizer)\n# Silence a warning.\ntokenizer.decode([tokenizer.eos_token_id]);","metadata":{"id":"IM5o_4w1hfyV","execution":{"iopub.status.busy":"2024-03-15T15:07:20.081419Z","iopub.execute_input":"2024-03-15T15:07:20.082305Z","iopub.status.idle":"2024-03-15T15:07:44.451187Z","shell.execute_reply.started":"2024-03-15T15:07:20.082265Z","shell.execute_reply":"2024-03-15T15:07:44.449971Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"354641b12ad1443c9ec9759e1e37538e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fe38fd9b576414b9d12f4c9298a7fdd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b2fa11af68a4d66a93e94af519e2150"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de1dd9a0b8b04870a18c4124aa4d0f07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8324e27bdd374def92dc68fd5c456a66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba75b2d8256143d5bc89d9ec531403ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7e2225a70394cd794549928ca81f972"}},"metadata":{}},{"name":"stderr","text":"2024-03-15 15:07:33.288123: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-15 15:07:33.288463: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-15 15:07:33.447996: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"token_to_id_dict = tokenizer.get_vocab()\nprint(f\"The tokenizer has {len(token_to_id_dict)} strings in its vocabulary.\")\nprint(f\"The model has {model.num_parameters():,d} parameters.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m-Z9_U0LUEVQ","outputId":"1d639faf-5b56-4bb2-81e5-054ee086ef0a","execution":{"iopub.status.busy":"2024-03-15T15:07:44.459202Z","iopub.execute_input":"2024-03-15T15:07:44.459722Z","iopub.status.idle":"2024-03-15T15:07:44.501336Z","shell.execute_reply.started":"2024-03-15T15:07:44.459680Z","shell.execute_reply":"2024-03-15T15:07:44.499985Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"The tokenizer has 50257 strings in its vocabulary.\nThe model has 124,439,808 parameters.\n","output_type":"stream"}]},{"cell_type":"code","source":"# warning: this assumes that there are no gaps in the token ids, which happens to be true for this tokenizer.\nid_to_token = [token for token, id in sorted(token_to_id_dict.items(), key=lambda x: x[1])]\nprint(f\"The first 10 tokens are: {id_to_token[:10]}\")\nprint(f\"The last 10 tokens are: {id_to_token[-10:]}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-15T15:07:44.503337Z","iopub.execute_input":"2024-03-15T15:07:44.504159Z","iopub.status.idle":"2024-03-15T15:07:44.690203Z","shell.execute_reply.started":"2024-03-15T15:07:44.504111Z","shell.execute_reply":"2024-03-15T15:07:44.688854Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"The first 10 tokens are: ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*']\nThe last 10 tokens are: ['Ġ(/', 'âĢ¦.\"', 'Compar', 'Ġamplification', 'ominated', 'Ġregress', 'ĠCollider', 'Ġinformants', 'Ġgazed', '<|endoftext|>']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Demo","metadata":{}},{"cell_type":"code","source":"set_seed(0)\nmodel.generate(\n    **tokenizer(\"A list of colors: red, blue,\", return_tensors=\"pt\"),\n    max_new_tokens=10, do_sample=True, temperature=0.3, penalty_alpha=.5, top_k=5, streamer=streamer);","metadata":{"execution":{"iopub.status.busy":"2024-03-15T15:07:44.691873Z","iopub.execute_input":"2024-03-15T15:07:44.692407Z","iopub.status.idle":"2024-03-15T15:07:45.299530Z","shell.execute_reply.started":"2024-03-15T15:07:44.692362Z","shell.execute_reply":"2024-03-15T15:07:45.298221Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":" A list of colors: red, blue, green, yellow, orange, yellow, orange,\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Task\n\nConsider the following phrase:","metadata":{"id":"OOUiz_PsUZgS"}},{"cell_type":"code","source":"phrase = \"I visited Muskegon\"\n# Another one to try later. This was a famous early example of the GPT-2 model:\n# phrase = \"In a shocking finding, scientists discovered a herd of unicorns living in\"","metadata":{"id":"JS7Z-DjoUiLK","execution":{"iopub.status.busy":"2024-03-15T15:07:45.301635Z","iopub.execute_input":"2024-03-15T15:07:45.302414Z","iopub.status.idle":"2024-03-15T15:07:45.308760Z","shell.execute_reply.started":"2024-03-15T15:07:45.302359Z","shell.execute_reply":"2024-03-15T15:07:45.307324Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Getting familiar with tokens\n\n1: Use `tokenizer.tokenize` to convert the phrase into a list of tokens. (What do you think the `Ġ` means?)","metadata":{}},{"cell_type":"code","source":"tokens = tokenizer.tokenize(phrase)\ntokens","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hyq-5XWSUx_8","outputId":"22efb7a8-37c5-46f0-e230-c3b8e5ad6bdc","execution":{"iopub.status.busy":"2024-03-15T15:07:45.310646Z","iopub.execute_input":"2024-03-15T15:07:45.311175Z","iopub.status.idle":"2024-03-15T15:07:45.326058Z","shell.execute_reply.started":"2024-03-15T15:07:45.311134Z","shell.execute_reply":"2024-03-15T15:07:45.323765Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"['ĠI', 'Ġvisited', 'ĠMus', 'ke', 'gon']"},"metadata":{}}]},{"cell_type":"markdown","source":"2: Use `tokenizer.convert_tokens_to_string` to convert the tokens back into a string.\n","metadata":{}},{"cell_type":"code","source":"tokenizer.convert_tokens_to_string(tokens)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T15:07:45.327870Z","iopub.execute_input":"2024-03-15T15:07:45.328329Z","iopub.status.idle":"2024-03-15T15:07:45.339656Z","shell.execute_reply.started":"2024-03-15T15:07:45.328298Z","shell.execute_reply":"2024-03-15T15:07:45.337886Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"' I visited Muskegon'"},"metadata":{}}]},{"cell_type":"code","source":"# for comparison:\n''.join(tokens)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T15:07:45.343467Z","iopub.execute_input":"2024-03-15T15:07:45.343886Z","iopub.status.idle":"2024-03-15T15:07:45.352704Z","shell.execute_reply.started":"2024-03-15T15:07:45.343855Z","shell.execute_reply":"2024-03-15T15:07:45.351281Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'ĠIĠvisitedĠMuskegon'"},"metadata":{}}]},{"cell_type":"markdown","source":"**What is the difference between the output from `convert_tokens_to_string` and the result of ''.join(tokens)?**\n<br>\n<br>Result of ''.join(tokens): 'ĠIĠvisitedĠMuskegon'\nResult from `convert_tokens_to_string`: ' I visited Muskegon'\n\nThe difference between these two results is that the output from `convert_tokens_to_string` has blank spots for its spaces, and the result of ''.join(tokens) inserts a G into each space.","metadata":{}},{"cell_type":"markdown","source":"3: Use `tokenizer.encode` to convert the original phrase into token ids. (*Note: this is equivalent to `tokenize` followed by `convert_tokens_to_ids`*.) Call the result `input_ids`.\n","metadata":{}},{"cell_type":"code","source":"input_ids = tokenizer.encode(phrase)\ninput_ids","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GkaoLSFMiHzb","outputId":"18c6391e-a9aa-4d4c-dace-d49f8bbcba7a","execution":{"iopub.status.busy":"2024-03-15T15:07:45.354569Z","iopub.execute_input":"2024-03-15T15:07:45.355596Z","iopub.status.idle":"2024-03-15T15:07:45.366530Z","shell.execute_reply.started":"2024-03-15T15:07:45.355550Z","shell.execute_reply":"2024-03-15T15:07:45.365376Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"[314, 8672, 2629, 365, 14520]"},"metadata":{}}]},{"cell_type":"markdown","source":"4: Turn `input_ids` back into a readable string. Try this two ways: (1) using `tokenizer.decode` and (2) using `convert_ids_to_tokens`. **The result of (1) should be the same as the result of (2).**","metadata":{}},{"cell_type":"code","source":"# using convert_ids_to_tokens\nconvert = tokenizer.convert_ids_to_tokens(input_ids)\ntokenizer.convert_tokens_to_string(convert)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T15:07:45.368077Z","iopub.execute_input":"2024-03-15T15:07:45.368515Z","iopub.status.idle":"2024-03-15T15:07:45.376680Z","shell.execute_reply.started":"2024-03-15T15:07:45.368481Z","shell.execute_reply":"2024-03-15T15:07:45.375497Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"' I visited Muskegon'"},"metadata":{}}]},{"cell_type":"code","source":"# using tokenizer.decode\ntokenizer.decode(input_ids)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"ncSRaBaZix8R","outputId":"204670f9-d7c4-4856-c804-a038b77ccd1c","execution":{"iopub.status.busy":"2024-03-15T15:07:45.378318Z","iopub.execute_input":"2024-03-15T15:07:45.378704Z","iopub.status.idle":"2024-03-15T15:07:45.390527Z","shell.execute_reply.started":"2024-03-15T15:07:45.378673Z","shell.execute_reply":"2024-03-15T15:07:45.389155Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"' I visited Muskegon'"},"metadata":{}}]},{"cell_type":"markdown","source":"### Applying what you learned\n\n5: Use `model.generate(input_ids_batch)` to generate a completion of this phrase. (Note that we needed to add `[]`s to give a \"batch\" dimension to the input, and convert the result to a PyTorch `tensor` for the model code to use it.) Call the result `output_ids`. This one is done for you.\n","metadata":{}},{"cell_type":"code","source":"input_ids_batch = tensor([input_ids])\noutput_ids = model.generate(input_ids_batch, max_new_tokens=20, do_sample=True, top_k=50)\noutput_ids","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5PZm3eIjjKCJ","outputId":"1c4b1a63-de00-44f9-eee7-85714012dbc6","execution":{"iopub.status.busy":"2024-03-15T15:07:45.393025Z","iopub.execute_input":"2024-03-15T15:07:45.393555Z","iopub.status.idle":"2024-03-15T15:07:46.259392Z","shell.execute_reply.started":"2024-03-15T15:07:45.393518Z","shell.execute_reply":"2024-03-15T15:07:46.258111Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"tensor([[  314,  8672,  2629,   365, 14520,   287,  9656,    13,   383,  3952,\n           373,  1363,   284,  1811,  1957, 17245,    11,  1390,  9935,  1709,\n          3266,   694,   290,  3941, 25732]])"},"metadata":{}}]},{"cell_type":"markdown","source":"6: Convert your `output_ids` into a readable form. (Note: it has an extra \"batch\" dimension, so you'll need to use `output_ids[0]`.)","metadata":{}},{"cell_type":"code","source":"readable = tokenizer.convert_ids_to_tokens(output_ids[0])\ntokenizer.convert_tokens_to_string(readable)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"2kKJ8rvijVez","outputId":"386df167-0e88-45f1-b1a0-01beab1f0bc8","execution":{"iopub.status.busy":"2024-03-15T15:07:46.261072Z","iopub.execute_input":"2024-03-15T15:07:46.261483Z","iopub.status.idle":"2024-03-15T15:07:46.271109Z","shell.execute_reply.started":"2024-03-15T15:07:46.261452Z","shell.execute_reply":"2024-03-15T15:07:46.269517Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"' I visited Muskegon in 1993. The park was home to several local musicians, including Dave Brubeck and Bill Kre'"},"metadata":{}}]},{"cell_type":"markdown","source":"Note: `generate` uses a greedy decoding by default, but it's highly customizable. We'll play more with it in later exercises. For now, if you want more interesting results, try:\n\n- Turn on `do_sample=True`. Run it a few times to see what it gives.\n- Set `top_k=5`. Or 50.","metadata":{}},{"cell_type":"markdown","source":"**When I turned on `do_sample=True`, I started to get results like:**<br>\n- \" I visited Muskegon two years ago, and had a wonderful experience with Mr. O'Brien. He had written a\"\n- ' I visited Muskegon in 1993. The park was home to several local musicians, including Dave Brubeck and Bill Kre'\n- \" I visited Muskegon on New Year's Eve. This was a strange place. We got up before sunrise and all the\"\n\n**When I set top_k=5, I started to get results like:**<br>\n- ' I visited Muskegon. I was in my 20s and was in the process of finishing a degree in English. I'\n- ' I visited Muskegon, the largest of the two rivers in North America, and was told that the water was \"so'\n- ' I visited Muskegon. I was there to visit the city of Muskegon, where the city of Muskegon'\n\n**When I set top_k=50, I started to get results like:**<br>\n- \" I visited Muskegon High School on August 17, 2017.\\n\\n'Our goal was to learn about the community from\"\n- ' I visited Muskegon in February to observe the wildlife in action, it was clear the situation was different.\\n\\nOn'\n- ' I visited Muskegon. I remember saying to the people there, \\'Well, what are they gonna do?\" and said'","metadata":{}},{"cell_type":"markdown","source":"7. What is the largest possible token id for this tokenizer? What token does it correspond to?","metadata":{}},{"cell_type":"code","source":"max_token_id = output_ids.max().item()\nprint(max_token_id)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T15:09:12.711121Z","iopub.execute_input":"2024-03-15T15:09:12.711578Z","iopub.status.idle":"2024-03-15T15:09:12.720842Z","shell.execute_reply.started":"2024-03-15T15:09:12.711544Z","shell.execute_reply":"2024-03-15T15:09:12.718909Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"25732\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The largest possbile token id for this tokenizer is 25,732. ","metadata":{}},{"cell_type":"markdown","source":"## Analysis","metadata":{}},{"cell_type":"markdown","source":"**Q1: Write a brief explanation of what a tokenizer does.** Note that we worked with two parts of a tokenizer in this exercise (one that deals only with strings, and another that deals with numbers); make sure your explanation addresses both parts.","metadata":{}},{"cell_type":"markdown","source":"A tokenizer has the ability to break text down into small pieces/sections called tokens. There are two main purposes that a tokenizer accomplishes.\n\n**Strings**: A tokenizer has the capability to break parts of text down, like a sentence or phrase, into individual tokens, like words or characters. \n\n**Numbers**: A tokenizer is also able to take each token(text has to be tokenized already) and map it to a unique numerical identifier, like an integer. This allows the model to process the information that we provided it numerically.","metadata":{}},{"cell_type":"markdown","source":"**Q2: Suppose a language model has learned to spell, e.g., after the prefix \"The word dog is spelled d o g\". Will it then already know how to spell any other word, or does it have to re-learn spelling for each word? Why or why not? (For example, try tokenizing the phrase \"The word walking is spelled: w a\" and then asking the LM to complete it.)**","metadata":{}},{"cell_type":"markdown","source":"No, I don't think that a language model will automatically learn to spell other words just because it's understands one. It needs to learn the spelling fo reach word individually because it doesn't understand words in the same way humans do","metadata":{}},{"cell_type":"markdown","source":"**Q3: Suppose you made a typo in your input. Explain what the tokenizer we used in this notebook will do with the new input. (Go back and try an input with a typo to see what happens.)**","metadata":{}},{"cell_type":"markdown","source":"If I were to make a typo in my input, the tokenizer would have a little bit of trouble figuring out what I was trying to say, especially if it was a big mistake. The model would try it's best to understand my input, but it would get a little confused if the mistake chages the text too much.","metadata":{}}]}