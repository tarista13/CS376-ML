{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11384,"sourceType":"modelInstanceVersion","modelInstanceId":6216},{"sourceId":26140,"sourceType":"modelInstanceVersion","modelInstanceId":22003}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Setup the environment\n#!pip install --upgrade huggingface_hub transformers\n\"\"\"\n!pip install bitsandbytes\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\naccess_token_read = UserSecretsClient().get_secret(\"HUGGINGFACE_TOKEN\")\nlogin(token = access_token_read)\n\"\"\"\n#!pip install git+https://github.com/huggingface/transformers -U\n#!pip install accelerate\n#!pip install -i https://pypi.org/simple/ bitsandbytes","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-04-17T07:33:13.005757Z","iopub.execute_input":"2024-04-17T07:33:13.006507Z","iopub.status.idle":"2024-04-17T07:33:13.020577Z","shell.execute_reply.started":"2024-04-17T07:33:13.006478Z","shell.execute_reply":"2024-04-17T07:33:13.019687Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'\\n!pip install bitsandbytes\\nfrom huggingface_hub import login\\nfrom kaggle_secrets import UserSecretsClient\\naccess_token_read = UserSecretsClient().get_secret(\"HUGGINGFACE_TOKEN\")\\nlogin(token = access_token_read)\\n'"},"metadata":{}}]},{"cell_type":"code","source":"import torch, os\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n# Work around a bug in the version of PyTorch and GPU hardware curretnly on Kaggle. On other hardware, removing these lines may lead to a speed-up.\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\n\n# Load the model\nUSE_INSTRUCTION_TUNED = True\nif USE_INSTRUCTION_TUNED:\n    model_name = '/kaggle/input/gemma/transformers/1.1-2b-it/1'\n    if not os.path.exists(model_name):\n        print(\"Warning: loading model weights from the Internet. This might take a bit of extra time.\")\n        model_name = \"google/gemma-1.1-2b-it\"\nelse:\n    model_name = \"/kaggle/input/gemma/transformers/2b/2\"\n    if not os.path.exists(model_name):\n        print(\"Warning: loading model weights from the Internet. This might take a bit of extra time.\")\n        model_name = \"google/gemma-2b\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map='auto',\n    torch_dtype=torch.bfloat16)\nstreamer = TextStreamer(tokenizer)\n# Silence a warning.\ntokenizer.decode([tokenizer.eos_token_id]);","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-04-17T07:33:13.022148Z","iopub.execute_input":"2024-04-17T07:33:13.022443Z","iopub.status.idle":"2024-04-17T07:34:41.798149Z","shell.execute_reply.started":"2024-04-17T07:33:13.022419Z","shell.execute_reply":"2024-04-17T07:34:41.797331Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b751b39d9fb8491e8c1af3e2ac0c2dfb"}},"metadata":{}},{"name":"stderr","text":"2024-04-17 07:34:32.943794: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-17 07:34:32.943905: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-17 07:34:33.065057: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Check where the whole model is loaded and what data type it's using.\nmodel.device, model.dtype","metadata":{"execution":{"iopub.status.busy":"2024-04-14T01:08:17.731844Z","iopub.execute_input":"2024-04-14T01:08:17.732385Z","iopub.status.idle":"2024-04-14T01:08:17.739082Z","shell.execute_reply.started":"2024-04-14T01:08:17.732344Z","shell.execute_reply":"2024-04-14T01:08:17.738072Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"(device(type='cuda', index=0), torch.bfloat16)"},"metadata":{}}]},{"cell_type":"code","source":"# Check where parameters are loaded. If this is anything other than {'': 0}\n# then probably some parts of the model got offloaded onto CPU and so will run slow.\nmodel.hf_device_map","metadata":{"execution":{"iopub.status.busy":"2024-04-14T01:08:17.740542Z","iopub.execute_input":"2024-04-14T01:08:17.740881Z","iopub.status.idle":"2024-04-14T01:08:17.805255Z","shell.execute_reply.started":"2024-04-14T01:08:17.740850Z","shell.execute_reply":"2024-04-14T01:08:17.804414Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"{'': 0}"},"metadata":{}}]},{"cell_type":"markdown","source":"## Warm-Up","metadata":{}},{"cell_type":"code","source":"%%time\ndoc = '''The capital of France is'''\nmodel_out = model.generate(\n    **tokenizer(doc, return_tensors='pt').to(model.device),\n    max_new_tokens=128,\n    do_sample=False,\n    streamer=streamer)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T01:08:17.807667Z","iopub.execute_input":"2024-04-14T01:08:17.807930Z","iopub.status.idle":"2024-04-14T01:08:21.366482Z","shell.execute_reply.started":"2024-04-14T01:08:17.807907Z","shell.execute_reply":"2024-04-14T01:08:21.365585Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"<bos>The capital of France is...\n\nA. Paris\nB. Lyon\nC. Marseille\nD. Bordeaux\n\nThe answer is A.\n\nThe capital of France is Paris. It is a major city and the political, economic, and cultural center of France.<eos>\nCPU times: user 2.87 s, sys: 187 ms, total: 3.06 s\nWall time: 3.55 s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\ndoc = '''Expression: 2 + 2. Result:'''\nmodel_out = model.generate(\n    **tokenizer(doc, return_tensors='pt').to(model.device),\n    max_new_tokens=128,\n    do_sample=False,\n    streamer=streamer)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T01:08:21.367746Z","iopub.execute_input":"2024-04-14T01:08:21.368183Z","iopub.status.idle":"2024-04-14T01:08:25.347407Z","shell.execute_reply.started":"2024-04-14T01:08:21.368148Z","shell.execute_reply":"2024-04-14T01:08:25.346388Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"<bos>Expression: 2 + 2. Result: 4\n\nThis is a simple mathematical expression that demonstrates the basic operations of addition.\n\n**Questions:**\n\n1. What are the different types of mathematical expressions?\n2. How are mathematical expressions used in real life?\n3. What are the benefits of using mathematical expressions?\n\n**Additional Information:**\n\n* Mathematical expressions can include numbers, variables, operators, and parentheses.\n* Mathematical expressions are used to represent and solve mathematical problems.\n* Mathematical expressions can be used in various fields, including science, engineering, finance, and education.\n\n**Answers:**\n\n**1. Different types of mathematical expressions:**\n\n- Algebraic\nCPU times: user 3.98 s, sys: 14.9 ms, total: 4 s\nWall time: 3.97 s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\ndoc = '''An expert Spanish translation of 'Language models are statistical models that can generate text.' is'''\nmodel_out = model.generate(\n    **tokenizer(doc, return_tensors='pt').to(model.device),\n    max_new_tokens=128,\n    do_sample=False,\n    streamer=streamer)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T01:08:25.348759Z","iopub.execute_input":"2024-04-14T01:08:25.349068Z","iopub.status.idle":"2024-04-14T01:08:28.640315Z","shell.execute_reply.started":"2024-04-14T01:08:25.349044Z","shell.execute_reply":"2024-04-14T01:08:28.639281Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"<bos>An expert Spanish translation of 'Language models are statistical models that can generate text.' is:\n\n* Modelo estadístico de generación de texto\n* Modelo de traducción estadística\n* Modelo de procesamiento del lenguaje estadístico\n* Modelo de análisis de datos estadísticos\n\nCan anyone please explain the difference between these options?\n\nI understand that \"Modelo estadístico de generación de texto\" and \"Modelo de procesamiento del lenguaje estadístico\" are related to statistical language models. But I'm not sure about the other two options.\n\nCould someone please explain the difference between these two options and provide additional context if necessary?<eos>\nCPU times: user 3.28 s, sys: 23.7 ms, total: 3.31 s\nWall time: 3.29 s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\ndoc = '''Please write me a function, called sum_evens, in python that takes in a list of number and outputs the sum of the even numbers in the list'''\nmodel_out = model.generate(\n    **tokenizer(doc, return_tensors='pt').to(model.device),\n    max_new_tokens=128,\n    do_sample=False,\n    streamer=streamer)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T01:08:28.641499Z","iopub.execute_input":"2024-04-14T01:08:28.641795Z","iopub.status.idle":"2024-04-14T01:08:32.474676Z","shell.execute_reply.started":"2024-04-14T01:08:28.641769Z","shell.execute_reply":"2024-04-14T01:08:32.473785Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"<bos>Please write me a function, called sum_evens, in python that takes in a list of number and outputs the sum of the even numbers in the list.\n\n```python\ndef sum_evens(list1):\n    sum = 0\n    for num in list1:\n        if num % 2 == 0:\n            sum += num\n    return sum\n```\n\n**Example Usage:**\n\n```python\nlist1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\nresult = sum_evens(list1)\n\nprint(result)  # Output: 30\n```\n\n**Explanation:**\n\n* The function takes in\nCPU times: user 3.83 s, sys: 17.8 ms, total: 3.85 s\nWall time: 3.83 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Answer Questions Below","metadata":{}},{"cell_type":"markdown","source":"1. Write a brief summary of how well Gemma performed on each task\n- USE_INSTRUCTION_TUNED = True\n  - r\n- USE_INSTRUCTION_TUNED = False\n  - r\n2. Notice that we expressed these tasks in the form of making reasonable completions of a document, not giving the model instructions. What implications does this have for how useful a language model is?\n- By expressing these tasks in the form of documents rather than instructions, we are able to get more out of the model's ability to generate coherent text based on context. This shows the model's ability to understand and produce human-like language.\n3. Notice that the model took some times to generate the first token, then generated the rest of the text more quickly(but still not instantly). Why do you think that is?\n- I think that the model took some time to generate the first token, then generated the rest of the text more quickly because it needs a little bit of time ot set up its processes. Once the model gets starts it's internal processes, it has the ability to generate the rest of the text more quickly becasue it's in a state where it can understand and process the input more efficiently","metadata":{}},{"cell_type":"markdown","source":"# Few-Shot Learning","metadata":{}},{"cell_type":"code","source":"%%time\ndoc = '''The capital of Michigan is Lansing. The capital of England is London. The capital of France is'''\nmodel_out = model.generate(\n    **tokenizer(doc, return_tensors='pt').to(model.device),\n    max_new_tokens=128,\n    do_sample=False,\n    streamer=streamer)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T01:08:32.475747Z","iopub.execute_input":"2024-04-14T01:08:32.476054Z","iopub.status.idle":"2024-04-14T01:08:34.051604Z","shell.execute_reply.started":"2024-04-14T01:08:32.476028Z","shell.execute_reply":"2024-04-14T01:08:34.050597Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"<bos>The capital of Michigan is Lansing. The capital of England is London. The capital of France is Paris.\n\nWhich of these is the odd one out?\n\nA) Lansing\nB) London\nC) Paris\n\nThe answer is B) London.\n\nLondon is the capital of England, not Michigan.<eos>\nCPU times: user 1.56 s, sys: 13.4 ms, total: 1.58 s\nWall time: 1.57 s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\ndoc= '''\nRequest: capital(\"Michigan\")\nResponse: \"Lansing\"\nRequest: capital(\"England\")\nResponse: capital(\"London\")\nRequest: capital(\"France\")\nResponse:'''\nmodel_out = model.generate(\n    **tokenizer(doc, return_tensors='pt').to(model.device),\n    max_new_tokens=128,\n    do_sample=False,\n    streamer=streamer)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T01:08:34.052770Z","iopub.execute_input":"2024-04-14T01:08:34.053079Z","iopub.status.idle":"2024-04-14T01:08:38.236966Z","shell.execute_reply.started":"2024-04-14T01:08:34.053051Z","shell.execute_reply":"2024-04-14T01:08:38.236064Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"<bos>\nRequest: capital(\"Michigan\")\nResponse: \"Lansing\"\nRequest: capital(\"England\")\nResponse: capital(\"London\")\nRequest: capital(\"France\")\nResponse: capital(\"Paris\"\n\nI understand that the provided code is for finding the capital of a country based on the user's input. However, the responses provided are not the actual capitals of the countries.\n\nTo fix this, we need to retrieve the actual capital names from reliable sources like official websites or reputable news organizations.\n\n**Modified Code:**\n\n```python\ndef get_capital(country):\n    \"\"\"\n    Returns the capital of a country.\n\n    Args:\n        country: The name of the country.\n\n    Returns:\n        The capital of the country.\n    \"\"\"\n\n    # Retrieve the capital\nCPU times: user 4.18 s, sys: 30.5 ms, total: 4.21 s\nWall time: 4.18 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Answer Question\nWrite a brief summary of how Gemma performed on this taks, as compared with not giving it any examples","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Chain of Thought","metadata":{}},{"cell_type":"code","source":"%%time\ndoc = '''I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?'''\nmodel_out = model.generate(\n    **tokenizer(doc, return_tensors='pt').to(model.device),\n    max_new_tokens=128,\n    do_sample=False,\n    streamer=streamer)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T01:08:38.239866Z","iopub.execute_input":"2024-04-14T01:08:38.240186Z","iopub.status.idle":"2024-04-14T01:08:39.403790Z","shell.execute_reply.started":"2024-04-14T01:08:38.240159Z","shell.execute_reply":"2024-04-14T01:08:39.402725Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"<bos>I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n\n**Answer:** 4 apples\n\nWhen you give away some apples, you don't actually receive them. You simply reduce the number you have.<eos>\nCPU times: user 1.16 s, sys: 9.49 ms, total: 1.17 s\nWall time: 1.16 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Answer Question\nWhat does Gemma predict?","metadata":{}},{"cell_type":"code","source":"%%time\ndoc = '''I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with? Let's think step by step. After I bought the apples, I had'''\nmodel_out = model.generate(\n    **tokenizer(doc, return_tensors='pt').to(model.device),\n    max_new_tokens=128,\n    do_sample=False,\n    streamer=streamer)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T01:08:39.405381Z","iopub.execute_input":"2024-04-14T01:08:39.405718Z","iopub.status.idle":"2024-04-14T01:08:40.896894Z","shell.execute_reply.started":"2024-04-14T01:08:39.405690Z","shell.execute_reply":"2024-04-14T01:08:40.896019Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"<bos>I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with? Let's think step by step. After I bought the apples, I had 10 apples. After I gave away 2 and gave to others, I had 8 apples. Then, I bought 5 more and ate 1, leaving me with 7 apples.<eos>\nCPU times: user 1.49 s, sys: 3.68 ms, total: 1.5 s\nWall time: 1.49 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Answer Question\n* How does the generated text change?","metadata":{}},{"cell_type":"markdown","source":"## Chat Templating","metadata":{}},{"cell_type":"code","source":"role = \"\"\"You are a helpful 2nd-grade teacher. Help a 2nd grader to answer questions in a short and clear manner.\"\"\"\ntask = \"\"\"Explain why the sky is blue\"\"\"\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": f\"{role}\\n\\n{task}\",\n    },\n ]\ntokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\nprint(tokenizer.batch_decode(tokenized_chat)[0])","metadata":{"execution":{"iopub.status.busy":"2024-04-14T01:36:32.311912Z","iopub.execute_input":"2024-04-14T01:36:32.312305Z","iopub.status.idle":"2024-04-14T01:36:32.319657Z","shell.execute_reply.started":"2024-04-14T01:36:32.312274Z","shell.execute_reply":"2024-04-14T01:36:32.318700Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"<bos><start_of_turn>user\nYou are a helpful 2nd-grade teacher. Help a 2nd grader to answer questions in a short and clear manner.\n\nExplain why the sky is blue<end_of_turn>\n<start_of_turn>model\n\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\ndoc = tokenizer.batch_decode(tokenized_chat)[0]\nmodel_out = model.generate(\n    **tokenizer(doc, return_tensors='pt').to(model.device),\n    max_new_tokens=128,\n    do_sample=False,\n    streamer=streamer)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T01:43:29.830166Z","iopub.execute_input":"2024-04-14T01:43:29.831163Z","iopub.status.idle":"2024-04-14T01:43:31.733432Z","shell.execute_reply.started":"2024-04-14T01:43:29.831128Z","shell.execute_reply":"2024-04-14T01:43:31.732621Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"<bos><bos><start_of_turn>user\nYou are a helpful 2nd-grade teacher. Help a 2nd grader to answer questions in a short and clear manner.\n\nExplain why the sky is blue<end_of_turn>\n<start_of_turn>model\nThe sky is blue because of a special type of particle called **cloud droplets**.\n\nWhen sunlight enters a cloud, the blue light is scattered more than other colors because it has a shorter wavelength. This means that when we look up at the sky, we see more blue light than any other color.<eos>\nCPU times: user 1.89 s, sys: 18.7 ms, total: 1.91 s\nWall time: 1.9 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Retrieval-Augmented Generation","metadata":{}},{"cell_type":"code","source":"import inspect\ndocstrings = {}\nfor name, obj in inspect.getmembers(torch.nn):\n    if inspect.isfunction(obj) or inspect.isclass(obj):\n        docstrings[name] = inspect.getdoc(obj)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T01:08:40.944276Z","iopub.execute_input":"2024-04-14T01:08:40.944702Z","iopub.status.idle":"2024-04-14T01:08:40.957599Z","shell.execute_reply.started":"2024-04-14T01:08:40.944677Z","shell.execute_reply":"2024-04-14T01:08:40.956625Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"docstrings.keys()","metadata":{"execution":{"iopub.status.busy":"2024-04-14T01:08:40.958763Z","iopub.execute_input":"2024-04-14T01:08:40.959028Z","iopub.status.idle":"2024-04-14T01:08:40.968080Z","shell.execute_reply.started":"2024-04-14T01:08:40.959005Z","shell.execute_reply":"2024-04-14T01:08:40.967237Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"dict_keys(['AdaptiveAvgPool1d', 'AdaptiveAvgPool2d', 'AdaptiveAvgPool3d', 'AdaptiveLogSoftmaxWithLoss', 'AdaptiveMaxPool1d', 'AdaptiveMaxPool2d', 'AdaptiveMaxPool3d', 'AlphaDropout', 'AvgPool1d', 'AvgPool2d', 'AvgPool3d', 'BCELoss', 'BCEWithLogitsLoss', 'BatchNorm1d', 'BatchNorm2d', 'BatchNorm3d', 'Bilinear', 'CELU', 'CTCLoss', 'ChannelShuffle', 'CircularPad1d', 'CircularPad2d', 'CircularPad3d', 'ConstantPad1d', 'ConstantPad2d', 'ConstantPad3d', 'Container', 'Conv1d', 'Conv2d', 'Conv3d', 'ConvTranspose1d', 'ConvTranspose2d', 'ConvTranspose3d', 'CosineEmbeddingLoss', 'CosineSimilarity', 'CrossEntropyLoss', 'CrossMapLRN2d', 'DataParallel', 'Dropout', 'Dropout1d', 'Dropout2d', 'Dropout3d', 'ELU', 'Embedding', 'EmbeddingBag', 'FeatureAlphaDropout', 'Flatten', 'Fold', 'FractionalMaxPool2d', 'FractionalMaxPool3d', 'GELU', 'GLU', 'GRU', 'GRUCell', 'GaussianNLLLoss', 'GroupNorm', 'Hardshrink', 'Hardsigmoid', 'Hardswish', 'Hardtanh', 'HingeEmbeddingLoss', 'HuberLoss', 'Identity', 'InstanceNorm1d', 'InstanceNorm2d', 'InstanceNorm3d', 'KLDivLoss', 'L1Loss', 'LPPool1d', 'LPPool2d', 'LSTM', 'LSTMCell', 'LayerNorm', 'LazyBatchNorm1d', 'LazyBatchNorm2d', 'LazyBatchNorm3d', 'LazyConv1d', 'LazyConv2d', 'LazyConv3d', 'LazyConvTranspose1d', 'LazyConvTranspose2d', 'LazyConvTranspose3d', 'LazyInstanceNorm1d', 'LazyInstanceNorm2d', 'LazyInstanceNorm3d', 'LazyLinear', 'LeakyReLU', 'Linear', 'LocalResponseNorm', 'LogSigmoid', 'LogSoftmax', 'MSELoss', 'MarginRankingLoss', 'MaxPool1d', 'MaxPool2d', 'MaxPool3d', 'MaxUnpool1d', 'MaxUnpool2d', 'MaxUnpool3d', 'Mish', 'Module', 'ModuleDict', 'ModuleList', 'MultiLabelMarginLoss', 'MultiLabelSoftMarginLoss', 'MultiMarginLoss', 'MultiheadAttention', 'NLLLoss', 'NLLLoss2d', 'PReLU', 'PairwiseDistance', 'Parameter', 'ParameterDict', 'ParameterList', 'PixelShuffle', 'PixelUnshuffle', 'PoissonNLLLoss', 'RNN', 'RNNBase', 'RNNCell', 'RNNCellBase', 'RReLU', 'ReLU', 'ReLU6', 'ReflectionPad1d', 'ReflectionPad2d', 'ReflectionPad3d', 'ReplicationPad1d', 'ReplicationPad2d', 'ReplicationPad3d', 'SELU', 'Sequential', 'SiLU', 'Sigmoid', 'SmoothL1Loss', 'SoftMarginLoss', 'Softmax', 'Softmax2d', 'Softmin', 'Softplus', 'Softshrink', 'Softsign', 'SyncBatchNorm', 'Tanh', 'Tanhshrink', 'Threshold', 'Transformer', 'TransformerDecoder', 'TransformerDecoderLayer', 'TransformerEncoder', 'TransformerEncoderLayer', 'TripletMarginLoss', 'TripletMarginWithDistanceLoss', 'Unflatten', 'Unfold', 'UninitializedBuffer', 'UninitializedParameter', 'Upsample', 'UpsamplingBilinear2d', 'UpsamplingNearest2d', 'ZeroPad1d', 'ZeroPad2d', 'ZeroPad3d', 'factory_kwargs'])"},"metadata":{}}]},{"cell_type":"code","source":"%%time\n\n# Define the specific task/question\ntask = \"Explain the options for TripletMarginLoss in PyTorch.\"\n\n# Function to retrieve the specific context from the docstrings based on the task\ndef retrieve_context(query):\n    relevant_context = []\n    for name, doc in docstrings.items():\n        if query.lower() in name.lower():\n            relevant_context.append(f\"{name}: {doc}\")\n    return ' '.join(relevant_context)  # Combine all relevant docstrings into a single string\nincorrect_context = docstrings['CrossEntropyLoss']\n\n# Retrieve the context for the given task\n# Format the document to be used as input to the model\ndoc = f'''{task}\n\nAnswer using the following context:\n{context}'''\n\n# Generate a response from the model\nmodel_out = model.generate(\n    **tokenizer(doc, return_tensors='pt').to(model.device),\n    max_new_tokens=128,\n    do_sample=False\n)\n\n# Decode the generated tokens to text\nresponse = tokenizer.decode(model_out[0], skip_special_tokens=True)\n\n# Print the response\nprint(response)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T02:02:24.207701Z","iopub.execute_input":"2024-04-14T02:02:24.208553Z","iopub.status.idle":"2024-04-14T02:02:28.715112Z","shell.execute_reply.started":"2024-04-14T02:02:24.208514Z","shell.execute_reply":"2024-04-14T02:02:28.714100Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Explain the options for TripletMarginLoss in PyTorch.\n\nAnswer using the following context:\nTripletMarginLoss: Creates a criterion that measures the triplet loss given an input\ntensors :math:`x1`, :math:`x2`, :math:`x3` and a margin with a value greater than :math:`0`.\nThis is used for measuring a relative similarity between samples. A triplet\nis composed by `a`, `p` and `n` (i.e., `anchor`, `positive examples` and `negative\nexamples` respectively). The shapes of all input tensors should be\n:math:`(N, D)`.\n\nThe distance swap is described in detail in the paper `Learning shallow\nconvolutional feature descriptors with triplet losses`_ by\nV. Balntas, E. Riba et al.\n\nThe loss function for each sample in the mini-batch is:\n\n.. math::\n    L(a, p, n) = \\max \\{d(a_i, p_i) - d(a_i, n_i) + {\\rm margin}, 0\\}\n\n\nwhere\n\n.. math::\n    d(x_i, y_i) = \\left\\lVert {\\bf x}_i - {\\bf y}_i \\right\\rVert_p\n\nThe norm is calculated using the specified p value and a small constant :math:`\\varepsilon` is\nadded for numerical stability.\n\nSee also :class:`~torch.nn.TripletMarginWithDistanceLoss`, which computes the\ntriplet margin loss for input tensors using a custom distance function.\n\nArgs:\n    margin (float, optional): Default: :math:`1`.\n    p (int, optional): The norm degree for pairwise distance. Default: :math:`2`.\n    eps (float, optional): Small constant for numerical stability. Default: :math:`1e-6`.\n    swap (bool, optional): The distance swap is described in detail in the paper\n        `Learning shallow convolutional feature descriptors with triplet losses` by\n        V. Balntas, E. Riba et al. Default: ``False``.\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n        when :attr:`reduce` is ``False``. Default: ``True``\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: ``True``\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n        ``'mean'``: the sum of the output will be divided by the number of\n        elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n        and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n        specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n\nShape:\n    - Input: :math:`(N, D)` or :math:`(D)` where :math:`D` is the vector dimension.\n    - Output: A Tensor of shape :math:`(N)` if :attr:`reduction` is ``'none'`` and\n      input shape is :math:`(N, D)`; a scalar otherwise.\n\nExamples::\n\n>>> triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2, eps=1e-7)\n>>> anchor = torch.randn(100, 128, requires_grad=True)\n>>> positive = torch.randn(100, 128, requires_grad=True)\n>>> negative = torch.randn(100, 128, requires_grad=True)\n>>> output = triplet_loss(anchor, positive, negative)\n>>> output.backward()\n\n.. _Learning shallow convolutional feature descriptors with triplet losses:\n    http://www.bmva.org/bmvc/2016/papers/paper119/index.html\n\n\n```\n\n**Options for TripletMarginLoss:**\n\n- **margin:** The margin parameter controls the width of the triplet loss. A larger margin will result in a wider loss surface and will be more sensitive to small changes in the triplet relationships.\n\n\n- **p:** The p parameter controls the norm degree for pairwise distance. A higher p value will result in a more complex loss surface that is better at capturing the relationships between the triplets.\n\n\n- **eps:** The eps parameter adds a small constant to the loss to make it more stable.\n\n\n- **swap:** This parameter determines whether to use the distance swap trick. The distance\nCPU times: user 4.5 s, sys: 2.61 ms, total: 4.5 s\nWall time: 4.5 s\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}